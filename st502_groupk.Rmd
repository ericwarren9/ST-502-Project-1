---
title: "ST 502 Project 1"
author: "Eric Warren, Chandler Ellsworth, Kevin Krupa"
date: "`r Sys.Date()`"
urlcolor: blue
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.pos = "H", out.extra = "")
options(scipen = 999)
```

```{r output setup, eval=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-502-Project-1/st502_groupk.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = TRUE, 
                toc_depth = 3,
                number_sections = TRUE,
                extra_dependencies = "float",
                always_allow_html = TRUE
                )
              )
```

# Background of Project

In this project, we are going to try to understand what good properties of confidence are. The main things we are going to consider are:

- Proportion of intervals that capture the true value (hopefully $1 - \alpha$)
- Proportion of intervals that miss above and proportion that miss below
- The average length of the interval

# Creation of Data

We are going to do this by comparing the performance of these intervals we are making for inference for various values of $p$ and $n$. To start of with we will want to generate some data for these different combinations of $p$ and $n$. The data has been generated which we will show briefly what it looks like.
```{r generate data}
# install.packages("tidyverse")
library(tidyverse)

# Initialize data
set.seed(999) # Allows us to reproduce results
p <- seq(from = .01, to = .99, length.out = 30) # Get values for p as combinations
n <- c(15, 100, 200) # Get values of n
combo <- expand.grid(p,n)
N <- 1500 # Set N to be the number of samples we want for each grouping

df <- data.frame()
temp <- data.frame()

for (i in n) {
  for (j in p) {
    temp <- data.frame(n = i, 
                       p = j, 
                       y_i = rbinom(N, size = i, prob = j)
                       )
    df <- rbind(df, temp)
  }
}

# This code just double checks to make sure we got 1500 values from each combination -- ALL should say 1500 for count
# df %>%
#   group_by(n, p) %>%
#   summarize(count = n())

# Show the first observations and save as a tibble
df <- df %>%
  mutate(p_hat = y_i / n)
head(df)

# Now show last observations
tail(df)
```

Please note for this data that it is in *long format* which means that each combination of $n$, $p$, and $y_i$ is shown as one row. So for example, if we want to find all the randomly generated data for the combination of $n = 15$ and $p = 0.01$, we would want to find the `r N` rows that show its corresponding $y_i$ values.

# Results

## Wald Confidence Intervals

Here we took a look at using the Wald Confidence Interval.
```{r wald ci}
# Get Wald Interval calculations for each n and p
expected_wald_ci <- df %>%
  mutate(
    alpha = .05,
    lower_ci = ifelse(p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) < 0, 0, p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n)),
    upper_ci = ifelse(p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) > 1, 1, p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n))
  ) %>%
  dplyr::select(-alpha)

# Make function to get transform our variables to see how well the CIs do
updateDF <- function(df) {
  df %>%
    mutate(count = ifelse((p < upper_ci) & (p > lower_ci), 1, 0),
           count_under = ifelse(p <= lower_ci, 1, 0),
           count_over = ifelse(p >= upper_ci, 1, 0)
           )
}

# Make function to get final df to use for analysis
final_summary <- function(data) {
  data %>%
    group_by(n, p) %>%
    summarize(prop = mean(count),
              num_under = mean(count_under),
              num_over = mean(count_over),
              int_length = mean(upper_ci - lower_ci)
              )
}

# Merge our data with df and expected_wald_ci to get all the information together
df_wald <- updateDF(df = expected_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_wald_proportion <- final_summary(df_wald)
```

## Adjusted Wald Interval

Here we took a look at using the Adjusted Wald Confidence Interval.
```{r adj wald ci}
# Function for Adj Wald Interval
expected_adj_wald_ci <- df %>%
  mutate(
    alpha = .05,
    p_hat2 = (y_i + 2) / (n + 4), # Get value of p from this approach
    lower_ci = ifelse(p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) < 0, 
                      0, 
                      p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n)),
    upper_ci = ifelse(p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) > 1, 
                      1, 
                      p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n))
  ) %>%
  dplyr::select(- c(alpha, p_hat2))

# Update our data with df and expected_adj_wald_ci to get all the information together
df_adj_wald <- updateDF(expected_adj_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_adj_wald_proportion <- final_summary(df_adj_wald)
```

## Clopper-Pearson (Exact) Interval

Here we took a look at using the Clopper-Pearson (Exact) Confidence Interval.
```{r exact ci}
# Get Exact Interval calculations for each n and p
expected_exact_ci <- df %>%
  mutate(
    alpha = 0.05,
    lower_ci = ifelse(
      y_i == 0, # If y_i is 0 then lower bound is 0
      0, 
      ifelse(
        y_i == n, # If y_i is n then lower bound is 1
        1, 
        ifelse(
          (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) < 0, # If expression is less than 0 then lower bound is 0
          0, 
          ifelse(
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) > 1, # If function is greater than 1 then lower bound is 1
            1, 
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) # Otherwise get bound
          )
        )
      )
    ),
    upper_ci = ifelse( # Note the criteria for ifelse is same as lower bound but now for upper bound
      y_i == 0, 
      0, 
      ifelse(
        y_i == n, 
        1, 
        ifelse(
          (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) < 0, 
          0, 
          ifelse(
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) > 1, 
            1, 
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1)
          )
        )
      )
    )
  ) %>%
  dplyr::select(-alpha)

# Update our data with df and expected_exact_ci to get all the information together
df_exact <- updateDF(expected_exact_ci)

# Make table that stores the values of proportion of contained in interval
df_exact_proportion <- final_summary(df_exact)
```

## Score Interval

Here we took a look at using the Score Confidence Interval.
```{r score ci}
# Function for Score Interval
expected_score_ci <- df %>%
  mutate(
    alpha = 0.05,
    z = qnorm(1 - (alpha / 2)), # z-score value
    left_side = p_hat + (z ** 2 / (2 * n)), # Estimate value before subtracting or adding
    right_side = z * sqrt(((p_hat * (1 - p_hat)) + (z ** 2 / (4 * n))) / n), # Everything in sqrt to add or subtract to original estimate
    denominator = 1 + (z ** 2 / n), # Divided by this value
    lower_ci = ifelse((left_side - right_side) / denominator < 0, 
                      0, 
                      (left_side - right_side) / denominator
                      ),
    upper_ci = ifelse((left_side + right_side) / denominator > 1,
                      1,
                      (left_side + right_side) / denominator
                      )
  ) %>%
  dplyr::select(-c(alpha, z, left_side, right_side, denominator)) # Remove what is not needed

# Update our data with df and expected_score_ci to get all the information together
df_score <- updateDF(expected_score_ci)

# Make table that stores the values of proportion of contained in interval
df_score_proportion <- final_summary(df_score)
```


## Raw Percentile Interval using a Parametric Bootstrap

Here we took a look at using the Raw Percentile Confidence Interval using a Parametric Bootstrap.
```{r}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Raw_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  raw <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #create CI based on quantiles of bootstrap resamples of p_hat
    CI <- quantile(p_hat_boot, c(alpha/2, 1-(alpha/2)))
    #change names of lower and upper CI
    names(CI) <- c('lower_ci', 'upper_ci')
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, CI))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = raw[1,],
           p = raw[2,],
           y_i = raw[3,],
           p_hat = raw[4,],
           lower_ci = raw[5,],
           upper_ci = raw[6,])
}
#use new function to find CI for each row of df based on raw percentile
raw <- Raw_CI(df)
#use updateDF function that calculates relevant count metrics
df_raw <- updateDF(raw)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_raw_proportion <- final_summary(df_raw)
```

## Bootstrap t Interval using a Parametric Bootstrap

Here we took a look at using the Bootstrap t Confidence Interval using a Parametric Bootstrap.
```{r}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Bootstrap_t_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  boot_t <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #throw out bootstrap resamples that give proportion either 0 of 1
    p_hat_boot <- p_hat_boot[((p_hat_boot != 0) & (p_hat_boot != 1))]
    #calculate t-statistic for each bootstrap resample
    t_star <- (p_hat_boot - p_hat) / sqrt((p_hat_boot*(1-p_hat_boot))/n)
    #find quantiles of approximate distribution of the t-statistic
    quantiles <- quantile(t_star, c(alpha/2, 1-(alpha/2)))
    #give appropriate names
    names(quantiles) <- c('lower_q', 'upper_q')
    #create appropriate lower and upper CI based on if observed binomial sample from df is either 0, 1, or neither
    lower_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['upper_q']*sd(p_hat_boot)))
    upper_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['lower_q']*sd(p_hat_boot)))
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, lower_ci, upper_ci))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = boot_t[1,],
             p = boot_t[2,],
             y_i = boot_t[3,],
             p_hat = boot_t[4,],
             lower_ci = boot_t[5,],
             upper_ci = boot_t[6,])
  }
#use new function to find CI for each row of df based on bootstrap t
bootstrap_t <- Bootstrap_t_CI(df)
#%>%filter((! is.na(lower_ci)) & (! is.na(upper_ci))) # Remove NA values from upper and lower bounds to get intervals
#use updateDF function that calculates relevant count metrics
df_bootstrap_t <- updateDF(bootstrap_t)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_bootstrap_t_proportion <- final_summary(df_bootstrap_t)
```

## Coverage Results

Sample size = 15
```{r}
#create list of all data frames from different confidence interval procedures
results <- list("Wald" = df_wald_proportion, "Adjusted Wald" = df_adj_wald_proportion, 
                "Exact" = df_exact_proportion, "Score" = df_score_proportion, 
                "Raw Percentile Bootstrap" = df_raw_proportion, "Bootstrap t" = df_bootstrap_t_proportion)
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 15
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion",
       xlab = "p",
       main = paste("Proportion containing with n =", n_, names(results)[i]))
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

Sample size = 100
```{r}
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 100
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion",
       xlab = "p",
       main = paste("Proportion containing with n =", n_, names(results)[i]))
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

Sample size = 200
```{r}
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 200
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion",
       xlab = "p",
       main = paste("Proportion containing with n =", n_, names(results)[i]))
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

## Average Confidence Interval Length Results for each sample size

```{r}
#loop through each sample size to create plot for each
for (n_ in n) {
  #create empty plot where lines of each avg CI length from each method can be added
  plot(NA, ylim = c(0,0.6), xlim = c(0,1), 
       #add appropriate labels
       ylab = "Average Length", xlab = "p", main = paste("Average length with n =",n_))
  #loop through number of data frames in results
  for (i in 1:6) {
    #store appropriate data frame
    df <- results[[i]]
    #filter for relevant sample size
    df <- df %>% 
      filter(n == n_)
    #plot the p values and corresponding avg CI length for current method
    lines(df$p, df$int_length, col = i, lwd = 2, add = TRUE)
  }
  #add legend that specifies each method
  legend(0.87, 0.62, legend = c("Wald","AdjWald","Exact","Score","Raw","Boott"), lty = 1, lwd = 2, col = 1:6, cex = 0.7)
}

```

















