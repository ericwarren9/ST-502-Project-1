---
title: "ST 502 Project 1"
author: "Eric Warren, Chandler Ellsworth, Kevin Krupa"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.pos = "H", out.extra = "")
options(scipen = 999)
```

```{r output setup, eval=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-502-Project-1/st502_groupk.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = TRUE, 
                toc_depth = 3,
                number_sections = TRUE,
                extra_dependencies = "float",
                always_allow_html = TRUE
                )
              )
```

# Goals of Project

In this report, it is our goal to compare the performance of confidence intervals of binomial proportions by making inference for various values of sample size ($n$) and success probability ($p$) based on a Monte Carlo simulation study. Six different methods will be used, and performance will be assessed by proportion of intervals that contain the true value of $p$, proportion of intervals that miss above or below $p$, and the average length of the interval. We hope to show through our analysis that approximate intervals are better than exact intervals and select the most favorable interval methods.

# Methods of Project

To conduct inference on $p$, six different confidence interval methods were used: Wald, Adjusted Wald, Score, Clopper-Pearson, Raw Percentile Parametric Bootstrap (Raw), and Bootstrap t Parametric Bootstrap (Boot t). The Wald, Adjusted Wald, and Score intervals are based on asymptotic normality assumptions, the differences being that Wald uses the sample proportion, Adjusted Wald adds two successes and two failures to the sample proportion, and Score attempts to correct for small sample sizes and extreme proportions. Clopper-Pearson is an exact confidence interval that, in theory, should give desired confidence levels for any $p$. Lastly, Raw and Boot t approximate confidence intervals based on empirical quantiles from bootstrap resampling distributions, where Raw approximates the sample proportion’s distribution, and Boot t mimics a “t-type” statistic.

# Creation of Data

The data was created by first identifying the ninety different combinations of $p$ and $n$. A loop was then used to draw 1500 independent samples ($y_i$) using the `rbinom()` function for each combination, with the results stored in a data frame with three columns: $n$, $p$, and $y_i$. Each data frame within the loop was combined with the previous, resulting in a consolidated data frame of the 1500 independent samples for each $n$ /$p$ combination. Lastly, $\hat{p_i} = \frac{Y_i}{n}$, the sample proportion, was then calculated for each sample. Below is a sample of the created data.
```{r generate data}
# install.packages(c("tidyverse", "knitr", "kableExtra")) # Uncomment if you do not have libraries
library(tidyverse)

# Initialize data
set.seed(999) # Allows us to reproduce results
p <- seq(from = .01, to = .99, length.out = 30) # Get values for p as combinations
n <- c(15, 100, 200) # Get values of n
combo <- expand.grid(p,n)
N <- 1500 # Set N to be the number of samples we want for each grouping

df <- data.frame()
temp <- data.frame()

for (i in n) {
  for (j in p) {
    temp <- data.frame(n = i, 
                       p = j, 
                       y_i = rbinom(N, size = i, prob = j)
                       )
    df <- rbind(df, temp)
  }
}

# This code just double checks to make sure we got 1500 values from each combination -- ALL should say 1500 for count
# df %>%
#   group_by(n, p) %>%
#   summarize(count = n())

# Create the p_hat variable
df <- df %>%
  mutate(p_hat = y_i / n)
```

```{r data shown, fig.align='center'}
# Have data in same row
par(mfrow = c(1, 2))
# Show the first and last observations 
knitr::kable(cbind(head(df), tail(df)), "latex") %>%
  kableExtra::kable_styling(position = "center",
                            latex_options = "HOLD_position")
```

Note the data is in *long format* which means each combination of $n$, $p$, $y_i$, $\hat{p_i}$ are shown as one row. For example, the combination of $n = 15$ and $p = 0.01$ would need to be referenced by the `r N` rows that show the corresponding $y_i$ / $\hat{p_i}$ values.

```{r wald ci}
# Get Wald Interval calculations for each n and p
expected_wald_ci <- df %>%
  mutate(
    alpha = .05,
    lower_ci = ifelse(p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) < 0, 0, p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n)),
    upper_ci = ifelse(p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) > 1, 1, p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n))
  ) %>%
  dplyr::select(-alpha)

# Make function to get transform our variables to see how well the CIs do
updateDF <- function(df) {
  df %>%
    mutate(count = ifelse((p < upper_ci) & (p > lower_ci), 1, 0),
           count_under = ifelse(p <= lower_ci, 1, 0),
           count_over = ifelse(p >= upper_ci, 1, 0)
           )
}

# Make function to get final df to use for analysis
final_summary <- function(data) {
  data %>%
    group_by(n, p) %>%
    summarize(prop = mean(count),
              num_under = mean(count_under),
              num_over = mean(count_over),
              int_length = mean(upper_ci - lower_ci)
              )
}

# Merge our data with df and expected_wald_ci to get all the information together
df_wald <- updateDF(df = expected_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_wald_proportion <- final_summary(df_wald)
```

```{r adj wald ci}
# Function for Adj Wald Interval
expected_adj_wald_ci <- df %>%
  mutate(
    alpha = .05,
    p_hat2 = (y_i + 2) / (n + 4), # Get value of p from this approach
    lower_ci = ifelse(p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) < 0, 
                      0, 
                      p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n)),
    upper_ci = ifelse(p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) > 1, 
                      1, 
                      p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n))
  ) %>%
  dplyr::select(- c(alpha, p_hat2))

# Update our data with df and expected_adj_wald_ci to get all the information together
df_adj_wald <- updateDF(expected_adj_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_adj_wald_proportion <- final_summary(df_adj_wald)
```

```{r exact ci}
# Get Exact Interval calculations for each n and p
expected_exact_ci <- df %>%
  mutate(
    alpha = 0.05,
    lower_ci = ifelse(
      y_i == 0, # If y_i is 0 then lower bound is 0
      0, 
      ifelse(
        y_i == n, # If y_i is n then lower bound is 1
        1, 
        ifelse(
          (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) < 0, # If expression is less than 0 then lower bound is 0
          0, 
          ifelse(
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) > 1, # If function is greater than 1 then lower bound is 1
            1, 
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) # Otherwise get bound
          )
        )
      )
    ),
    upper_ci = ifelse( # Note the criteria for ifelse is same as lower bound but now for upper bound
      y_i == 0, 
      0, 
      ifelse(
        y_i == n, 
        1, 
        ifelse(
          (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) < 0, 
          0, 
          ifelse(
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) > 1, 
            1, 
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1)
          )
        )
      )
    )
  ) %>%
  dplyr::select(-alpha)

# Update our data with df and expected_exact_ci to get all the information together
df_exact <- updateDF(expected_exact_ci)

# Make table that stores the values of proportion of contained in interval
df_exact_proportion <- final_summary(df_exact)
```

```{r score ci}
# Function for Score Interval
expected_score_ci <- df %>%
  mutate(
    alpha = 0.05,
    z = qnorm(1 - (alpha / 2)), # z-score value
    left_side = p_hat + (z ** 2 / (2 * n)), # Estimate value before subtracting or adding
    right_side = z * sqrt(((p_hat * (1 - p_hat)) + (z ** 2 / (4 * n))) / n), # Everything in sqrt to add or subtract to original estimate
    denominator = 1 + (z ** 2 / n), # Divided by this value
    lower_ci = ifelse((left_side - right_side) / denominator < 0, 
                      0, 
                      (left_side - right_side) / denominator
                      ),
    upper_ci = ifelse((left_side + right_side) / denominator > 1,
                      1,
                      (left_side + right_side) / denominator
                      )
  ) %>%
  dplyr::select(-c(alpha, z, left_side, right_side, denominator)) # Remove what is not needed

# Update our data with df and expected_score_ci to get all the information together
df_score <- updateDF(expected_score_ci)

# Make table that stores the values of proportion of contained in interval
df_score_proportion <- final_summary(df_score)
```

```{r bootstrap raw}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Raw_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  raw <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #create CI based on quantiles of bootstrap resamples of p_hat
    CI <- quantile(p_hat_boot, c(alpha/2, 1-(alpha/2)))
    #change names of lower and upper CI
    names(CI) <- c('lower_ci', 'upper_ci')
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, CI))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = raw[1,],
           p = raw[2,],
           y_i = raw[3,],
           p_hat = raw[4,],
           lower_ci = raw[5,],
           upper_ci = raw[6,])
}
#use new function to find CI for each row of df based on raw percentile
raw <- Raw_CI(df)
#use updateDF function that calculates relevant count metrics
df_raw <- updateDF(raw)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_raw_proportion <- final_summary(df_raw)
```

```{r bootstrap t}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Bootstrap_t_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  boot_t <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #throw out bootstrap resamples that give proportion either 0 of 1
    p_hat_boot <- p_hat_boot[((p_hat_boot != 0) & (p_hat_boot != 1))]
    #calculate t-statistic for each bootstrap resample
    t_star <- (p_hat_boot - p_hat) / sqrt((p_hat_boot*(1-p_hat_boot))/n)
    #find quantiles of approximate distribution of the t-statistic
    quantiles <- quantile(t_star, c(alpha/2, 1-(alpha/2)))
    #give appropriate names
    names(quantiles) <- c('lower_q', 'upper_q')
    #create appropriate lower and upper CI based on if observed binomial sample from df is either 0, n, or neither
    lower_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['upper_q']*sd(p_hat_boot)))
    upper_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['lower_q']*sd(p_hat_boot)))
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, lower_ci, upper_ci))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = boot_t[1,],
             p = boot_t[2,],
             y_i = boot_t[3,],
             p_hat = boot_t[4,],
             lower_ci = boot_t[5,],
             upper_ci = boot_t[6,])
  }
#use new function to find CI for each row of df based on bootstrap t
bootstrap_t <- Bootstrap_t_CI(df)
#%>%filter((! is.na(lower_ci)) & (! is.na(upper_ci))) # Remove NA values from upper and lower bounds to get intervals
#use updateDF function that calculates relevant count metrics
df_bootstrap_t <- updateDF(bootstrap_t)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_bootstrap_t_proportion <- final_summary(df_bootstrap_t)
```

# Calculating Quantities

Using the created data, 95% confidence intervals (setting $\alpha = 0.05$) were calculated from each of the independent samples for each method using the following interval formulas:
*Wald*: $\hat{p} \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}}$  
*Adjusted Wald*: $\hat{p} = \frac{y_i + 2}{n + 4}$ using the same interval formula as the **Wald Interval**  
*Score*: $\frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} \pm z_{\alpha/2} \sqrt{\frac{\frac{\hat{p} (1 - \hat{p}) + z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}$  
*Clopper-Pearson* (Exact): ($(1 + \frac{n - y_i + 1}{y_i F_{2 y_i, 2(n - y_i + 1), 1 - \frac{\alpha}{2}}})^{-1}$, $(1 + \frac{n - y_i}{(y_i + 1) F_{2 (y_i + 1), 2(n - y_i), \frac{\alpha}{2}}})^{-1}$) where $F_{a, b, c}$ denotes the $1 - c$ quantile from the F-distribution with degrees of freedom $a$ and $b$  
*Raw*: (theta_star_lower, theta_star_upper) where theta_star_lower and theta_star_upper are the alpha/2 and 1-alpha/2 quantiles of the bootstrap distribution of p_hat  
*Boot t*: (theta_hat - delta_upper x SE_hat(estimator), theta_hat - delta_lower x SE_hat(estimator)) where delta_lower and delta upper are the quantiles from the bootstrap distribution of T = p_hat - p / SE_hat(estimator)

Each confidence interval across the different methods was evaluated to see if the true $p$ was contained in the interval, or if it missed above or below, by classifying a 1 or 0 in three new indicator columns, “count,” “count_under,” and “count_over.” This enabled proportions to be calculated for each of the new columns by taking the average of the count from the 1500 independent samples for each $n$/$p$ combination. A similar procedure was done to calculate the average interval length, but instead of taking an average of the count, the average difference was taken between the upper and lower bound of the confidence interval. 

# Results

## Proportion of Intervals Capturing Parameter for Each Sample Size

```{r n is 15 graphs}
#create list of all data frames from different confidence interval procedures
results <- list("Wald" = df_wald_proportion, "Adjusted Wald" = df_adj_wald_proportion, 
                "Exact" = df_exact_proportion, "Score" = df_score_proportion, 
                "Raw Bootstrap" = df_raw_proportion, "Bootstrap t" = df_bootstrap_t_proportion)
# Make the plots in a 2 rows by 3 columns output
par(mfrow = c(2, 3))
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 15
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

As we can see for our lower sample size of $n = 15$, the Adjusted Wald seems to be performing the best of capturing our necessary proportion of confidence intervals that should capture our true proportion parameter $p$. The Score interval also seems to do a good job as well (minus the extremes) and the Exact interval also seems to do fairly well (with the "extreme" range is a little wider than the Score interval). The other interval methods do a poor job of correctly classifying our desired proportion of correct confidence intervals.

```{r n is 100 graphs}
# Make the plots in a 2 rows by 3 columns output
par(mfrow = c(2, 3))
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 100
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

As we can see for our medium sample size of $n = 100$, the Adjusted Wald again seems to be performing the best of capturing our necessary proportion of confidence intervals that should capture our true proportion parameter $p$ with the Score interval closely behind (minus the extremes being slightly below par). The other four intervals seem to do fairly well (with the "extreme" range is a little wider than the Score interval) but from these plots alone it is hard to say that one of those four are much "better" than the others.

```{r n is 200 graphs}
# Make the plots in a 2 rows by 3 columns output
par(mfrow = c(2, 3))
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 200
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95,lwd = 1.5)
}
```

As we can see for our large sample size of $n = 200$, the Adjusted Wald and Score intervals both seem to be performing the better than its "competitor" interval methods of capturing our necessary proportion of confidence intervals that should capture our true proportion parameter $p$ with the Score interval closely behind. The Exact and Wald intervals are close behind with just the extreme $p$ (true proportion values) having some slight faults. The other two bootstrap intervals seem to do fairly alright (with the "extreme" range is a little wider than the other intervals) and seem to be a little more inconsistent as getting to our ideal confidence level (which we set at 95%).

## Average Confidence Interval Length Results for Each Sample Size

```{r average lengths graphs}
# Make the plots in a 1 row by 3 columns output
par(mfrow = c(1, 3))
#loop through each sample size to create plot for each
for (n_ in n) {
  #create empty plot where lines of each avg CI length from each method can be added
  plot(NA, ylim = c(0,0.65), xlim = c(0,1), 
       #add appropriate labels
       ylab = "Average Length", xlab = "p", main = paste("Average length with n =",n_))
  #loop through number of data frames in results
  for (i in 1:6) {
    #store appropriate data frame
    df <- results[[i]]
    #filter for relevant sample size
    df <- df %>% 
      filter(n == n_)
    #plot the p values and corresponding avg CI length for current method
    lines(df$p, df$int_length, col = i, lwd = 2, add = TRUE)
  }
  #add legend that specifies each method
  legend(0.35, 
         0.67, 
         legend = c("Wald",
                    "Adjusted Wald",
                    "Exact",
                    "Score",
                    "Raw Bootstrap",
                    "Bootstrap T"
                    ), 
         lty = 1, 
         lwd = 2, 
         col = 1:6, 
         cex = 0.7
         )
}
```

For small samples (in our case $n = 15$), the bootstrap methods and the Score interval both seem to limit our confidence ranges, which we prefer intervals with smaller length (or less error in the bounds). For our medium size sample of $n = 100$, they are all pretty similar and for our large sample of $n = 200$, we can see very similar lengths with the bootstrap methods actually limiting our error slightly better than the other four methods. 

## Key Takeaways

Based on our computed graphs, there are several results that we can identify. When $n = 15$, the Adjusted-Wald and Score methods appear to have the best overall coverage in terms of achieving our desired confidence level. We can make the argument that the Adjusted-Wald method is better, because it achieves the desired level at the extreme values of $p$. With that being said, the average lengths for the Score method do appear to be smaller than the Adjusted-Wald interval lengths. This could explain why the Score method does not achieve the desired confidence level at the extreme levels of p. Moving to when $n = 100$, we can still consider the Adjusted-Wald and Score Methods to be the best in terms of overall coverage. The same argument for the Adjusted-Wald method can be made when discussing coverage at extreme levels of p, but this time the average length distributions for each method appear to be very similar. Finally, when $n = 200$, we can still see that Adjusted-Wald and Score methods have the best coverage; however, they are now very similar at the extremes, so a “best” distinction would be hard to make. Also, the distributions of average interval lengths once again are approximately the same for all six methods, so no distinction can be made here either.

# Conclusions

After reviewing the results, we have come to the following conclusions for the “best” interval method. In terms of overall results, the best performing methods are the Adjusted-Wald and Score Methods. Regarding smaller sample sizes, the Score method appears to perform the best overall, as it does well in reaching our ideal confidence level and limits the length of our error bounds. Now, with that being said, the Adjusted-Wald interval obtained the desired confidence level, even at the extremes where the Score method will miss just slightly. We can see for lower sample sizes when we are looking at the lower true proportion $p$ the other four methods tend to perform horribly when capturing the parameter in our confidence levels. On top of that, the Wald Interval tends to perform poorly when evaluating low sample sizes, but will perform much better as we increase our sample size. However, the average interval length for the Score method is narrower than the Adjusted-Wald, meaning we will get a more precise interval. Therefore, for smaller sample sizes, we recommend using the Score method when producing a Confidence Interval, because we only miss our confidence level slightly for very extreme values of $p$. This decision was referenced in the **Results** section. Now as the sample size increases (such as $n = 100$ or $n = 200$) we will recommend using the Adjusted-Wald interval. The Adjusted-Wald interval will always achieve our desired confidence level, whereas the Score Interval will hover at or just below it. Also, the average interval lengths for the six methods are approximately the same for all values of $p$, so no distinction on “best” method can truly be made, based on strictly looking at interval length. Therefore, we will put much more weight towards the fact that the Adjusted-Wald method will achieves our desired confidence level, even in the event of an extreme $p$ (where $p$ is close to 0 or 1). In conclusion, when producing confidence intervals, we will recommend using the Score method for smaller sample sizes and the Adjusted-Wald method for larger sample sizes. 