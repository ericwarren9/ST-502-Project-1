---
title: "ST 502 Project 1"
author: "Eric Warren, Chandler Ellsworth, Kevin Krupa"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
header-includes:
  - \usepackage{float}
  - \pagenumbering{gobble}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.pos = "H", out.extra = "")
options(scipen = 999)
```

```{r output setup, eval=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-502-Project-1/st502_groupk.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = FALSE, 
                toc_depth = 3,
                number_sections = TRUE,
                extra_dependencies = "float",
                always_allow_html = TRUE,
                pagenumbering = "gobble"
                )
              )
```

# Goals of Project

In this report, it is our goal to compare the performance of confidence intervals of binomial proportions by making inference for various values of sample size ($n$) and success probability ($p$) based on a Monte Carlo simulation study. Six different methods will be used, and performance will be assessed by proportion of intervals that contain the true value of $p$, proportion of intervals that miss above or below $p$, and the average length of the interval. We hope to show through our analysis that approximate intervals are better than exact intervals and select the most favorable interval methods.

# Methods of Project

To conduct inference on $p$, six different confidence interval methods were used: Wald, Adjusted Wald, Score, Clopper-Pearson, Raw Percentile Parametric Bootstrap (Raw), and Bootstrap t Parametric Bootstrap (Boot t). The Wald, Adjusted Wald, and Score intervals are based on asymptotic normality assumptions, the differences being that Wald uses the sample proportion, Adjusted Wald adds two successes and two failures to the sample proportion, and Score attempts to correct for small sample sizes and extreme proportions. Clopper-Pearson is an exact confidence interval that, in theory, should give desired confidence levels for any $p$. Lastly, Raw and Boot t approximate confidence intervals based on empirical quantiles from bootstrap resampling distributions, where Raw approximates the sample proportion’s distribution, and Boot t mimics a “t-type” statistic.

# Creation of Data

The data was created by first identifying the ninety different combinations of $p$ and $n$. A loop was then used to draw 1500 independent samples ($y_i$) using the `rbinom()` function for each combination, with the results stored in a data frame with three columns: $n$, $p$, and $y_i$. Each data frame within the loop was combined with the previous, resulting in a consolidated data frame of the 1500 independent samples for each $n$ /$p$ combination. Lastly, $\hat{p_i} = \frac{Y_i}{n}$, the sample proportion, was then calculated for each sample. Below is a sample of the created data.
```{r generate data}
# install.packages(c("tidyverse", "knitr", "kableExtra")) # Uncomment if you do not have libraries
library(tidyverse)

# Initialize data
set.seed(999) # Allows us to reproduce results
p <- seq(from = .01, to = .99, length.out = 30) # Get values for p as combinations
n <- c(15, 100, 200) # Get values of n
combo <- expand.grid(p,n)
N <- 1500 # Set N to be the number of samples we want for each grouping

df <- data.frame()
temp <- data.frame()

for (i in n) {
  for (j in p) {
    temp <- data.frame(n = i, 
                       p = j, 
                       y_i = rbinom(N, size = i, prob = j)
                       )
    df <- rbind(df, temp)
  }
}

# This code just double checks to make sure we got 1500 values from each combination -- ALL should say 1500 for count
# df %>%
#   group_by(n, p) %>%
#   summarize(count = n())

# Create the p_hat variable
df <- df %>%
  mutate(p_hat = y_i / n)
```

```{r data shown, fig.align='center'}
# Have data in same row
par(mfrow = c(1, 2))
# Show the first and last observations 
knitr::kable(cbind(head(df), tail(df)), "latex") %>%
  kableExtra::kable_styling(position = "center",
                            latex_options = "HOLD_position")
```

Note the data is in *long format* which means each combination of $n$, $p$, $y_i$, $\hat{p_i}$ are shown as one row. For example, the combination of $n = 15$ and $p = 0.01$ would need to be referenced by the `r N` rows that show the corresponding $y_i$ / $\hat{p_i}$ values.

```{r wald ci}
# Get Wald Interval calculations for each n and p
expected_wald_ci <- df %>%
  mutate(
    alpha = .05,
    lower_ci = ifelse(p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) < 0, 0, p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n)),
    upper_ci = ifelse(p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) > 1, 1, p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n))
  ) %>%
  dplyr::select(-alpha)

# Make function to get transform our variables to see how well the CIs do
updateDF <- function(df) {
  df %>%
    mutate(count = ifelse((p < upper_ci) & (p > lower_ci), 1, 0),
           count_under = ifelse(p <= lower_ci, 1, 0),
           count_over = ifelse(p >= upper_ci, 1, 0)
           )
}

# Make function to get final df to use for analysis
final_summary <- function(data) {
  data %>%
    group_by(n, p) %>%
    summarize(prop = mean(count),
              num_under = mean(count_under),
              num_over = mean(count_over),
              int_length = mean(upper_ci - lower_ci)
              )
}

# Merge our data with df and expected_wald_ci to get all the information together
df_wald <- updateDF(df = expected_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_wald_proportion <- final_summary(df_wald)
```

```{r adj wald ci}
# Function for Adj Wald Interval
expected_adj_wald_ci <- df %>%
  mutate(
    alpha = .05,
    p_hat2 = (y_i + 2) / (n + 4), # Get value of p from this approach
    lower_ci = ifelse(p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) < 0, 
                      0, 
                      p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n)),
    upper_ci = ifelse(p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) > 1, 
                      1, 
                      p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n))
  ) %>%
  dplyr::select(- c(alpha, p_hat2))

# Update our data with df and expected_adj_wald_ci to get all the information together
df_adj_wald <- updateDF(expected_adj_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_adj_wald_proportion <- final_summary(df_adj_wald)
```

```{r exact ci}
# Get Exact Interval calculations for each n and p
expected_exact_ci <- df %>%
  mutate(
    alpha = 0.05,
    lower_ci = ifelse(
      y_i == 0, # If y_i is 0 then lower bound is 0
      0, 
      ifelse(
        y_i == n, # If y_i is n then lower bound is 1
        1, 
        ifelse(
          (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) < 0, # If expression is less than 0 then lower bound is 0
          0, 
          ifelse(
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) > 1, # If function is greater than 1 then lower bound is 1
            1, 
            (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) # Otherwise get bound
          )
        )
      )
    ),
    upper_ci = ifelse( # Note the criteria for ifelse is same as lower bound but now for upper bound
      y_i == 0, 
      0, 
      ifelse(
        y_i == n, 
        1, 
        ifelse(
          (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) < 0, 
          0, 
          ifelse(
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) > 1, 
            1, 
            (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1)
          )
        )
      )
    )
  ) %>%
  dplyr::select(-alpha)

# Update our data with df and expected_exact_ci to get all the information together
df_exact <- updateDF(expected_exact_ci)

# Make table that stores the values of proportion of contained in interval
df_exact_proportion <- final_summary(df_exact)
```

```{r score ci}
# Function for Score Interval
expected_score_ci <- df %>%
  mutate(
    alpha = 0.05,
    z = qnorm(1 - (alpha / 2)), # z-score value
    left_side = p_hat + (z ** 2 / (2 * n)), # Estimate value before subtracting or adding
    right_side = z * sqrt(((p_hat * (1 - p_hat)) + (z ** 2 / (4 * n))) / n), # Everything in sqrt to add or subtract to original estimate
    denominator = 1 + (z ** 2 / n), # Divided by this value
    lower_ci = ifelse((left_side - right_side) / denominator < 0, 
                      0, 
                      (left_side - right_side) / denominator
                      ),
    upper_ci = ifelse((left_side + right_side) / denominator > 1,
                      1,
                      (left_side + right_side) / denominator
                      )
  ) %>%
  dplyr::select(-c(alpha, z, left_side, right_side, denominator)) # Remove what is not needed

# Update our data with df and expected_score_ci to get all the information together
df_score <- updateDF(expected_score_ci)

# Make table that stores the values of proportion of contained in interval
df_score_proportion <- final_summary(df_score)
```

```{r bootstrap raw}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Raw_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  raw <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #create CI based on quantiles of bootstrap resamples of p_hat
    CI <- quantile(p_hat_boot, c(alpha/2, 1-(alpha/2)))
    #change names of lower and upper CI
    names(CI) <- c('lower_ci', 'upper_ci')
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, CI))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = raw[1,],
           p = raw[2,],
           y_i = raw[3,],
           p_hat = raw[4,],
           lower_ci = raw[5,],
           upper_ci = raw[6,])
}
#use new function to find CI for each row of df based on raw percentile
raw <- Raw_CI(df)
#use updateDF function that calculates relevant count metrics
df_raw <- updateDF(raw)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_raw_proportion <- final_summary(df_raw)
```

```{r bootstrap t}
#create confidence interval function that takes in a data frame of n,p combinations and their observed y_i and p_hat
Bootstrap_t_CI <- function(df, B = 200, alpha = 0.05) {#create defaults for # bootstrap resamples and alpha
  boot_t <- apply(df, MARGIN = 1, FUN = function(combo) {#create CI for every row in the data frame
    #set seed for reproducibility
    set.seed(999)
    #store current parameter values and sample values
    n <- combo['n']
    p <- combo['p']
    y_i <- combo['y_i']
    p_hat <- combo['p_hat']
    #create B bootstrap resamples using estimate of p
    y_boot <- rbinom(B, size = n, prob = p_hat)
    #calculate p_hat for each bootstrap resample
    p_hat_boot <- y_boot / n
    #throw out bootstrap resamples that give proportion either 0 of 1
    p_hat_boot <- p_hat_boot[((p_hat_boot != 0) & (p_hat_boot != 1))]
    #calculate t-statistic for each bootstrap resample
    t_star <- (p_hat_boot - p_hat) / sqrt((p_hat_boot*(1-p_hat_boot))/n)
    #find quantiles of approximate distribution of the t-statistic
    quantiles <- quantile(t_star, c(alpha/2, 1-(alpha/2)))
    #give appropriate names
    names(quantiles) <- c('lower_q', 'upper_q')
    #create appropriate lower and upper CI based on if observed binomial sample from df is either 0, n, or neither
    lower_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['upper_q']*sd(p_hat_boot)))
    upper_ci <- ifelse(y_i == 0, 0, 
                       ifelse(y_i == n, 1,
                              p_hat - quantiles['lower_q']*sd(p_hat_boot)))
    #return values needed for similar format to df provided in function call
    return(c(n, p, y_i, p_hat, lower_ci, upper_ci))
    })
  #create a data frame that is in same format as df provided in function call
  df <- data.frame(n = boot_t[1,],
             p = boot_t[2,],
             y_i = boot_t[3,],
             p_hat = boot_t[4,],
             lower_ci = boot_t[5,],
             upper_ci = boot_t[6,])
  }
#use new function to find CI for each row of df based on bootstrap t
bootstrap_t <- Bootstrap_t_CI(df)
#%>%filter((! is.na(lower_ci)) & (! is.na(upper_ci))) # Remove NA values from upper and lower bounds to get intervals
#use updateDF function that calculates relevant count metrics
df_bootstrap_t <- updateDF(bootstrap_t)
#use final_summary function to group by n,p combinations and get relevant performance metrics
df_bootstrap_t_proportion <- final_summary(df_bootstrap_t)
```

# Calculating Quantities

Using the created data, 95% confidence intervals (setting $\alpha = 0.05$) were calculated from each of the independent samples for each method using the following interval formulas:  
**Wald**: $\hat{p} \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}}$  
**Adjusted Wald**: $\hat{p} = \frac{y_i + 2}{n + 4}$ using the same interval formula as the Wald Interval  
**Score**: $\frac{\hat{p} + \frac{z_{\alpha/2}^2}{2n} \pm z_{\alpha/2} \sqrt{\frac{\frac{\hat{p} (1 - \hat{p}) + z_{\alpha/2}^2}{4n}}{n}}}{1 + \frac{z_{\alpha/2}^2}{n}}$  
**Clopper-Pearson** (Exact): ($(1 + \frac{n - y_i + 1}{y_i F_{2 y_i, 2(n - y_i + 1), 1 - \frac{\alpha}{2}}})^{-1}$, $(1 + \frac{n - y_i}{(y_i + 1) F_{2 (y_i + 1), 2(n - y_i), \frac{\alpha}{2}}})^{-1}$) where $F_{a, b, c}$ denotes the $1 - c$ quantile from the F-distribution with degrees of freedom $a$ and $b$  
**Raw**: ($\theta_{lower}^*$, $\theta_{upper}^*$) where $\theta_{lower}^*$ and $\theta_{upper}^*$ are the $\frac{\alpha}{2}$ and $1 - \frac{\alpha}{2}$ quantiles of the bootstrap distribution of $\hat{p}$  
**Boot t**: ($\hat{p} - \delta_{upper} * \hat{SE}(\hat{p}), \hat{p} - \delta_{lower} * \hat{SE}(\hat{p})$) where $\delta_{lower}$ and $\delta_{upper}$ are the quantiles from the bootstrap distribution of $T = \frac{\hat{p} - p}{\hat{SE}(\hat{p})}$

Each confidence interval across the different methods was evaluated to see if the true $p$ was contained in the interval, or if it missed above or below, by classifying a 1 or 0 in three new indicator columns, “count,” “count_under,” and “count_over.” This enabled proportions to be calculated for each of the new columns by taking the average of the count from the 1500 independent samples for each $n$ and $p$ combination. A similar procedure was done to calculate the average interval length, but instead of taking an average of the count, the average difference was taken between the upper and lower bound of the confidence interval. 

# Results

```{r n plots}
#create list of all data frames from different confidence interval procedures
results <- list("Wald" = df_wald_proportion, "Adjusted Wald" = df_adj_wald_proportion, 
                "Exact" = df_exact_proportion, "Score" = df_score_proportion, 
                "Raw Bootstrap" = df_raw_proportion, "Bootstrap t" = df_bootstrap_t_proportion)

# Make the plots in a 2 rows by 3 columns output for n = 15
par(mfrow = c(2, 3))
#loop through number of data frames in results
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 15
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95, lwd = 1.5)
}

# Make the plots in a 2 rows by 3 columns output for n = 100
par(mfrow = c(2, 3))
# Loop through number of data frames in results for n = 100
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 100
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95, lwd = 1.5)
}

# Make the plots in a 2 rows by 3 columns output for n = 200
par(mfrow = c(2, 3))
# Loop through number of data frames in results for n = 200
for (i in 1:6) {
  #store appropriate data frame
  df <- results[[i]]
  #filter for relevant sample size
  n_ <- 200
  df <- df %>% 
    filter(n == n_)
  #plot the p values and corresponding coverage proportion results
  plot(df$p, df$prop, col = i, type = "l", lwd = 2,
       ylim = c(0,1), xlim = c(0,1),
       #add appropriate labels
       ylab = "Proportion Containing p",
       xlab = "p",
       main = paste("Proportion Containing p \n with n =", 
                    n_, 
                    "for \n",
                    names(results)[i],
                    "Interval")
       )
  #add line for nominal confidence level
  abline(h = 0.95, lwd = 1.5)
}
```

From the calculations and plots, there were clear trends of increased performance as the sample increased across all methods, but certain methods performed better than others for a set sample size. First, for a small sample size of $n = 15$, the Score interval method performed the best in relation to the proportion of confidence intervals that captured the true $p$ value relative to our desired proportion of 0.95. The Adjusted Wald interval also had stable proportions across the support of $p$, with better results for extreme values close to 0 or 1, but generally had proportions greater than desired. The Exact interval did well for $p$ between 0.3 to 0.7, but had significantly lower proportions than desired for other $p$ values. The other interval methods did a poor job of creating confidence intervals that contained the true $p$ based on our desired level of confidence. For a medium sample size of $n = 100$, the Adjusted Wald and Score interval methods seemed to perform the best in relation to proportion contained relative to desired, with the other four methods closely behind aside from $p$ values close to 0 or 1. Lastly, for a large sample size of $n = 200$, the Adjusted Wald and Score intervals again performed well. The Wald and Exact methods had comparable results to the Adjusted Wald, except had lower proportions than desired for $p$ values close to 0 to 1, while Adjusted Wald had more conservative results with proportions greater than desired for the same $p$ range. For larger sample sizes, it became more apparent the Raw method outperformed the Boot t method for $p$ values closer to 0 and 1 in relation to having proportions closer to desired of 0.95. 

```{r length plots}
# Make the plots in a 1 row by 3 columns output
par(mfrow = c(1, 3))
#loop through each sample size to create plot for each
for (n_ in n) {
  #create empty plot where lines of each avg CI length from each method can be added
  plot(NA, ylim = c(0,0.65), xlim = c(0,1), 
       #add appropriate labels
       ylab = "Average Length", xlab = "p", main = paste("Average length with n =",n_))
  #loop through number of data frames in results
  for (i in 1:6) {
    #store appropriate data frame
    df <- results[[i]]
    #filter for relevant sample size
    df <- df %>% 
      filter(n == n_)
    #plot the p values and corresponding avg CI length for current method
    lines(df$p, df$int_length, col = i, lwd = 2, add = TRUE)
  }
  #add legend that specifies each method
  legend(0.35, 
         0.67, 
         legend = c("Wald",
                    "Adjusted Wald",
                    "Exact",
                    "Score",
                    "Raw Bootstrap",
                    "Bootstrap T"
                    ), 
         lty = 1, 
         lwd = 2, 
         col = 1:6, 
         cex = 0.7
         )
}
```

For small sample sizes of $n = 15$, the Bootstrap methods and the Score method limited our interval ranges for the same level of confidence, which is preferred. For medium sample sizes of $n = 100$, all methods were similar, while for even larger sample sizes of $n = 200$, the Bootstrap methods had slightly lower average interval lengths for $p$ in the range of 0.3 to 0.7. Factoring the average interval length results into the results from how well the confidence intervals captured the true $p$, the Score method appeared even more favorable for smaller sample sizes.

```{r over under plot}
# Get adj wald over-under splits by n
adj_wald_splits <- df_adj_wald_proportion %>%
  group_by(n) %>%
  summarize(over = mean(num_over),
            under = mean(num_under)) %>%
  pivot_longer(cols = over:under, 
               names_to = "error_type", 
               values_to = "freq") %>%
  mutate(name = "Adjusted Wald")

# Get score over-under splits by n
score_splits <- df_score_proportion %>%
  group_by(n) %>%
  summarize(over = mean(num_over),
            under = mean(num_under)) %>%
  pivot_longer(cols = over:under, 
               names_to = "error_type", 
               values_to = "freq") %>%
  mutate(name = "Score")

# Make together into one df
splits <- rbind(adj_wald_splits, score_splits)

# Make over-under splits plot
splits %>%
  ggplot(aes(x = as.factor(n), y = freq, fill = error_type)) +
  geom_bar(stat = "identity", position = "dodge") + 
  facet_wrap(~ name) + 
  labs(title = "Error for Types of Intervals Selected", 
       x = "Sample Size",
       y = "Proportion of Interval 'Missing' Either High or Low",
       fill = "Type of Error") +
  theme_bw()
```

Since the Adjusted Wald and Score intervals appeared to perform the best in relation to proportion of intervals containing the true $p$, while also having comparable average interval lengths relative to the other methods, we lastly checked the proportion of intervals that either overestimated or underestimated the true $p$ for these two methods. Across all sample sizes for the two methods, the overestimated and underestimated proportions were about the same, meaning the interval estimation procedures were empirically unbiased and were reasonable for estimating intervals that contained the true $p$. The results also showed the general preference towards the Score method since, if the proportion of intervals that contained the true $p$ were close to the desired proportion of 0.95, that would leave 0.05 to be separated between overestimated or underestimated. The preference for the remaining 0.5 would be to have it equally split amongst overestimated and underestimated at 0.025, which the Score interval more closely aligned to over the Adjusted Wald, especially for smaller sample sizes. This also showed the Adjusted Wald had slightly more conservative estimations, since the remaining proportion that did not contain was less than 0.05 for all sample sizes, likely indicated by a greater average interval length.

# Conclusions

After reviewing the results, our preferred methods for creating confidence intervals of binomial proportions are the Adjusted Wald and Score Methods. These are our preferred methods since they had the best coverage and adequate interval lengths for our desired confidence level for all sample sizes, across the support of $p$. Further, these “approximate” methods performed better than the exact interval method, Clopper-Pearson, and had a much friendlier interpretation and calculation. Adjusted Wald and Score are especially preferred for lower samples sizes, as the other four methods performed poorly, especially when $p$ was close to 0 or 1. We slightly favor the Score method over the Adjusted Wald for small sample sizes as the Score method performed better for values of $p$ close to 0 and 1 and had a more narrow average interval length, meaning a more precise interval of $p$ could be given with the same level of confidence. For large sample sizes, we are unbiased towards using the Adjusted Wald or Score method, but if the true $p$ was assumed closer to 0 or 1,  one may prefer to use the Adjusted Wald over Score since the Adjusted Wald had more conservative coverage for these values with a similar average interval length. 