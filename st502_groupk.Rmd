---
title: "ST 502 Project 1"
author: "Eric Warren, Chandler Ellsworth, Kevin Krupa"
date: "`r Sys.Date()`"
urlcolor: blue
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.pos = "H", out.extra = "")
options(scipen = 999)
```

```{r output setup, eval=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-502-Project-1/st502_groupk.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = TRUE, 
                toc_depth = 3,
                number_sections = TRUE,
                extra_dependencies = "float",
                always_allow_html = TRUE
                )
              )
```

# Background of Project

In this project, we are going to try to understand what good properties of confidence are. The main things we are going to consider are:

- Proportion of intervals that capture the true value (hopefully $1 - \alpha$)
- Proportion of intervals that miss above and proportion that miss below
- The average length of the interval

# Creation of Data

We are going to do this by comparing the performance of these intervals we are making for inference for various values of $p$ and $n$. To start of with we will want to generate some data for these different combinations of $p$ and $n$. The data has been generated which we will show briefly what it looks like.
```{r generate data}
# install.packages("tidyverse")
library(tidyverse)

# Initialize data
set.seed(999) # Allows us to reproduce results
p <- seq(from = .01, to = .99, length.out = 30) # Get values for p as combinations
n <- c(15, 100, 200) # Get values of n
combo <- expand.grid(p,n)
N <- 1500 # Set N to be the number of samples we want for each grouping

df <- data.frame()
temp <- data.frame()

for (i in n) {
  for (j in p) {
    temp <- data.frame(n = i, 
                       p = j, 
                       y_i = rbinom(N, size = i, prob = j)
                       )
    df <- rbind(df, temp)
  }
}

# This code just double checks to make sure we got 1500 values from each combination -- ALL should say 1500 for count
# df %>%
#   group_by(n, p) %>%
#   summarize(count = n())

# Show the first observations and save as a tibble
df <- df %>%
  mutate(p_hat = y_i / n)
head(df)

# Now show last observations
tail(df)
```

Please note for this data that it is in *long format* which means that each combination of $n$, $p$, and $y_i$ is shown as one row. So for example, if we want to find all the randomly generated data for the combination of $n = 15$ and $p = 0.01$, we would want to find the `r N` rows that show its corresponding $y_i$ values.

# Results

## Wald Confidence Intervals

Here we took a look at using the Wald Confidence Interval.
```{r wald ci}
# Get Wald Interval calculations for each n and p
expected_wald_ci <- df %>%
  mutate(
    alpha = .05,
    lower_ci = ifelse(p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) < 0, 0, p_hat - qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n)),
    upper_ci = ifelse(p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n) > 1, 1, p_hat + qnorm(1 - (alpha / 2)) * sqrt((p_hat * (1 - p_hat)) / n))
  ) %>%
  dplyr::select(-alpha)

# Make function to get transform our variables to see how well the CIs do
updateDF <- function(df) {
  df %>%
    mutate(count = ifelse((p < upper_ci) & (p > lower_ci), 1, 0),
           count_under = ifelse(p <= lower_ci, 1, 0),
           count_over = ifelse(p >= upper_ci, 1, 0)
           )
}

# Make function to get final df to use for analysis
final_summary <- function(data) {
  data %>%
    group_by(n, p) %>%
    summarize(prop = mean(count),
              num_under = mean(count_under),
              num_over = mean(count_over),
              int_length = mean(upper_ci - lower_ci)
              )
}

# Merge our data with df and expected_wald_ci to get all the information together
df_wald <- updateDF(df = expected_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_wald_proportion <- final_summary(df_wald)
```

## Adjusted Wald Interval

Here we took a look at using the Adjusted Wald Confidence Interval.
```{r adj wald ci}
# Function for Adj Wald Interval
expected_adj_wald_ci <- df %>%
  mutate(
    alpha = .05,
    p_hat2 = (y_i + 2) / (n + 4), # Get value of p from this approach
    lower_ci = ifelse(p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) < 0, 
                      0, 
                      p_hat2 - qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n)),
    upper_ci = ifelse(p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n) > 1, 
                      1, 
                      p_hat2 + qnorm(1 - (alpha / 2)) * sqrt((p_hat2 * (1 - p_hat2)) / n))
  ) %>%
  dplyr::select(- c(alpha, p_hat2))

# Merge our data with df and expected_adj_wald_ci to get all the information together
df_adj_wald <- updateDF(expected_adj_wald_ci)

# Make table that stores the values of proportion of contained in interval
df_adj_wald_proportion <- final_summary(df_adj_wald)
```

## Clopper-Pearson (Exact) Interval

Here we took a look at using the Clopper-Pearson (Exact) Confidence Interval.
```{r exact ci}
# Get Exact Interval calculations for each n and p
expected_exact_ci <- df %>%
  mutate(
    alpha = 0.05,
    lower_ci = ifelse(
      ((y_i == 0) | ((1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1) < 0)),
      0,
      (1 + ((n - y_i + 1) / (y_i * qf(1 - (alpha / 2), 2 * y_i, 2 * (n - y_i + 1), lower.tail = F)))) ** (-1)
    ),
    upper_ci = ifelse(
      ((y_i == n) | ((1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1) > 1)),
      1,
      (1 + ((n - y_i) / ((y_i + 1) * qf(alpha / 2, 2 * (y_i + 1), 2 * (n - y_i), lower.tail = F)))) ** (-1)
    )
  ) %>%
  dplyr::select(-alpha)

# Merge our data with df and expected_exact_ci to get all the information together
df_exact <- updateDF(expected_exact_ci)

# Make table that stores the values of proportion of contained in interval
df_exact_proportion <- final_summary(df_exact)
```

## Score Interval

Here we took a look at using the Score Confidence Interval.
```{r score ci}
# Function for Score Interval
expected_score_ci <- df %>%
  mutate(
    alpha = 0.05,
    z = qnorm(1 - (alpha / 2)), # z-score value
    left_side = p_hat + (z ** 2 / (2 * n)), # Estimate value before subtracting or adding
    right_side = z * sqrt(((p_hat * (1 - p_hat)) + (z ** 2 / (4 * n))) / n), # Everything in sqrt to add or subtract to original estimate
    denominator = 1 + (z ** 2 / n), # Divided by this value
    lower_ci = ifelse((left_side - right_side) / denominator < 0, 
                      0, 
                      (left_side - right_side) / denominator
                      ),
    upper_ci = ifelse((left_side + right_side) / denominator > 1,
                      1,
                      (left_side + right_side) / denominator
                      )
  ) %>%
  dplyr::select(-c(alpha, z, left_side, right_side, denominator)) # Remove what is not needed

# Merge our data with df and expected_score_ci to get all the information together
df_score <- updateDF(expected_score_ci)

# Make table that stores the values of proportion of contained in interval
df_score_proportion <- final_summary(df_score)
```


## Raw Percentile Interval

```{r}
N_ <- 100
B <- 50

raw <- apply(combo, MARGIN = 1, FUN = function(param) {
  names(param) <- c('p','n')
  n <-  param['n']
  p <- param['p']
  boot_estimates <- replicate(B, {
    Y <- rbinom(N_, size = n, prob = p)
    p_hat_boot <- Y / n
    CI <- quantile(p_hat_boot, c(0.025, 0.975))
    names(CI) <- c('lower_ci', 'upper_ci')
    return(c(p = p, n = n, CI))
    })
  df <- data.frame(p = boot_estimates[1,],
           n = boot_estimates[2,],
           lower_ci = boot_estimates[3,],
           upper_ci = boot_estimates[4,])
  return(df)
})

raw <- bind_rows(raw)

df_raw <- updateDF(raw)

(df_raw_proportion <- final_summary(df_raw))

```


```{r}
df_wald_proportion
```

















